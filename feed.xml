<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://teng-gao.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://teng-gao.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-22T19:53:39+00:00</updated><id>https://teng-gao.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">What makes doing science fulfilling?</title><link href="https://teng-gao.github.io/blog/2023/fulfillment/" rel="alternate" type="text/html" title="What makes doing science fulfilling?"/><published>2023-12-22T00:00:00+00:00</published><updated>2023-12-22T00:00:00+00:00</updated><id>https://teng-gao.github.io/blog/2023/fulfillment</id><content type="html" xml:base="https://teng-gao.github.io/blog/2023/fulfillment/"><![CDATA[<meta name="twitter:card" content="summary"/> <meta name="twitter:site" content="@tGaoTeng"/> <meta name="twitter:title" content="What makes doing science fulfilling?"/> <meta name="twitter:description" content="Some reflections on the necessary conditions that make it worthwhile to do science"/> <p>Doing science is not always a fulfilling experience. It is hard, risky, and time consuming. You also don’t get rich from doing it. So what are some essential ingredients that need to be present for it to be ultimately meaningful and worthwhile? Here I reflect on this question and list a few things that I think are important.</p> <ol> <li><strong>You’re working on high impact problems in an area that you deeply care about.</strong> Academia is competitive, and it’s often easy to choose a project or a lab thinking “if I do this, then I’ll be successful”. Personally, I find that over time this way of thinking really eats at my passion, makes science less enjoyable, and detracts me from my real purpose. For what is a successful career if doesn’t change the world in ways that you want?</li> <li><strong>In the process of doing so, you’re growing as a scientist</strong>, either by acquiring new skills or becoming the expert in a key domain that will enable you to solve even bigger problems and have broader impact in your future work. Research is risky and not every project succeeds. However, don’t forget that your growth as a scientist is a project in and of itself, and it should never fail!</li> <li><strong>You’re learning from an amazing mentor/team who constantly inspire you.</strong> Makes the day to day more fun and key to your growth as discussed above.</li> <li><strong>You’re solving a problem that you are uniquely equipped to solve</strong>. I often ask myself: there’s so much progress and innovation happening in science everyday, why is it important that I work on the things that I do? Is there any real way for an individual to influence the trajectory of science? I think one way to answer yes to this question is to find problems that you’re uniquely positioned to solve based on your background, training, or skill set. In other words, you want to find something that, if you didn’t choose to work on this, it probably will not happen for quite some time. Interestingly, this also makes doing something where you’re “racing to be the first” almost meaningless, because then you’re working on something that will happen anyway.</li> <li><strong>There is a community who also care about the problem you’re solving</strong> and benefit from the approach you develop or build on discoveries that you make. This helps amplify the impact of your work.</li> </ol>]]></content><author><name></name></author><category term="reflections"/><summary type="html"><![CDATA[Some reflections on the necessary conditions that make it worthwhile to do science]]></summary></entry><entry><title type="html">Principles for new method development</title><link href="https://teng-gao.github.io/blog/2023/method/" rel="alternate" type="text/html" title="Principles for new method development"/><published>2023-02-03T00:00:00+00:00</published><updated>2023-02-03T00:00:00+00:00</updated><id>https://teng-gao.github.io/blog/2023/method</id><content type="html" xml:base="https://teng-gao.github.io/blog/2023/method/"><![CDATA[<meta name="twitter:card" content="summary"/> <meta name="twitter:site" content="@tGaoTeng"/> <meta name="twitter:title" content="Principles for new method development"/> <meta name="twitter:description" content="Self-notes for new method development in computational biology"/> <meta name="twitter:image" content="https://teng-gao.github.io/assets/img/wheel.jpg"/> <p>“Progress in science depends on new techniques, new discoveries and new ideas, probably in that order.” - Sydney Brenner</p> <p>Developing a new computational method for genomics is exciting but can also be daunting at first. It seems to be as much an art as it is an exact science. Here I summarize some lessons learned having gone through the whole process once:</p> <ol> <li><strong>Avoid reinventing the wheel.</strong> Reuse existing solutions that people have tested before and innovate on top of them.</li> <li><strong>Simplicity is key.</strong> Fancy math does not always lead to better results. Before applying any complex techniques, try a simple one first.</li> <li><strong>Iterate fast and optimize later.</strong> Experimental code does not have to be tidy or efficient (it just has to be correct). Optimize once you know something works.</li> <li><strong>Establish a benchmark early on.</strong> A benchmark is not only essential for demonstrating performance improvements in the end, but also for testing out various approaches while developing a tool.</li> <li><strong>If you can’t solve a big problem, solve a subpart first.</strong> Similarly, if you can’t solve a general problem, first solve a specific case. Recognize inter-dependencies of sub-problems and modularize. Eventually you will get there!</li> <li><strong>Be systematic.</strong> I spent a lot of time adjusting model parameters on an ad-hoc basis in order to make my method work better for specific cases. What really helped me understand how things work was systematically studying the effect of different parameter values across all possible scenarios.</li> <li><strong>Abstraction is the best way to make a method generalize.</strong> I remember a time when I kept finding new cases where my method breaks down. Instead of addressing each problem individually, it was much better to summarize them into classes of errors, and solve them all at once by extending the model.</li> </ol>]]></content><author><name></name></author><category term="genomics"/><category term="reflections"/><summary type="html"><![CDATA[Self-notes for new method development in computational biology]]></summary></entry><entry><title type="html">Allele-specific CNV analysis from spatial transcriptomics</title><link href="https://teng-gao.github.io/blog/2023/spatial/" rel="alternate" type="text/html" title="Allele-specific CNV analysis from spatial transcriptomics"/><published>2023-01-02T17:39:00+00:00</published><updated>2023-01-02T17:39:00+00:00</updated><id>https://teng-gao.github.io/blog/2023/spatial</id><content type="html" xml:base="https://teng-gao.github.io/blog/2023/spatial/"><![CDATA[]]></content><author><name></name></author><category term="tutorial"/><category term="genomics"/><summary type="html"><![CDATA[Numbat for spatial transcriptomics]]></summary></entry><entry><title type="html">How to derive an EM algorithm from scratch</title><link href="https://teng-gao.github.io/blog/2022/ems/" rel="alternate" type="text/html" title="How to derive an EM algorithm from scratch"/><published>2022-11-02T00:00:00+00:00</published><updated>2022-11-02T00:00:00+00:00</updated><id>https://teng-gao.github.io/blog/2022/ems</id><content type="html" xml:base="https://teng-gao.github.io/blog/2022/ems/"><![CDATA[<meta name="twitter:card" content="summary"/> <meta name="twitter:site" content="@tGaoTeng"/> <meta name="twitter:title" content="How to derive an EM algorithm from scratch"/> <meta name="twitter:description" content="From theory to implementation"/> <meta name="twitter:image" content="https://teng-gao.github.io/assets/img/EM1.png"/> <p>Expectation-maximization (EM) is a powerful class of statistical algorithms for performing inference in the presence of latent (unobserved) variables. There are many variations of EM being applied to solve different problems (e.g. Gaussian mixtures, HMMs, LDA, you name it). However, it is often unclear how to derive an EM algorithm, from scratch, for a new problem. In this tutorial, we will first derive and prove the <em>general</em> EM framework, which reveals the fundamental principles that all EM algorithms share. Then, we will use an example (mixture of exponentials) to illustrate how to derive a <em>specific</em> EM algorithm using this general framework for any problem.</p> <h2 id="the-motivation-behind-em">The motivation behind EM</h2> <p>Maximum likelihood estimation (MLE) is a widely used parameter estimation technique in statistical inference. MLE is relatively straightforward to perform in the settings of <em>complete</em> observations. However, in practice, we frequently have to account for unobserved variables/parameters in the model, which we call <strong>latent variables</strong>. In these cases, the likelihood of our (observed) data actually depends on the values of the latent variables.</p> <p>Formally, let $x = {x_1, x_2, \ldots}$ be the set of observed data and $z = {z_1, z_2, \ldots}$ be the (unobserved) latent variables. Let $\theta$ be the parameter that we wish to estimate and assume each $z_i \in \mathcal{Z}$ is discrete. To find the MLE of $\theta$, we need to maximize the log <strong>marginal likelihood</strong> of the data:</p> \[l(\theta) = \log p(x|\theta) = \log \Big(\sum_{z\in\mathcal{Z}^n} p(x, z|\theta) \Big) = \log \Big(\sum_{z\in\mathcal{Z}^n} p(x|z,\theta)p(z|\theta) \Big)\] <p>which is often intractable (since the space of all possible combinations of $z$ can be immense) or has no closed-form solution (since we have a summation inside a log, which makes partial derivatives with respect to $\theta$ difficult).</p> <p>Note that we can always treat the likelihood as a generic objective function and apply different numerical optimization techniques. EM is an iterative algorithm that solves this optimization problem faster by exploiting the probabilistic structure of the data generation process.</p> <h2 id="the-general-em-framework">The general EM framework</h2> <p>Since all EM algorithms are just specific realizations of the general EM algorithm, we will first derive the general EM framework on the most abstract level (also from scratch!).</p> <h3 id="derivation">Derivation</h3> <p>The key idea behind EM is that although there is no easy way to optimize the marginal log-likelihood $l(\theta)=\log p(x|\theta)$ directly, we can construct a <em>lower bound</em> of it that we can easily optimize. Let $q$ be any valid distribution function for $z$,</p> \[\begin{align*} l(\theta) = \log p(x|\theta) &amp;= \log \sum_{z \in \mathcal{Z}^n} p(x,z|\theta) \\ &amp;= \log \sum_{z \in \mathcal{Z}^n} q(z)\frac{ p(x,z|\theta)}{q(z)} \\ &amp;= \log E_{q(z)}\Big[\frac{ p(x,z|\theta)}{q(z)} \Big] \\ &amp;\geq E_{q(z)}\Big[\log \frac{p(x,z|\theta)}{q(z)} \Big] \\ &amp;= E_{q(z)} [\log p(x,z|\theta)] - E_{q(z)}[\log q(z)] \\ &amp;= E_{q(z)} [\log p(x,z|\theta)] + H(q) \\ &amp;\equiv \text{ELBO}(\theta, q) \end{align*}\] <p>In the fourth step we applied Jensen’s inequality (and the fact that log is a concave function). You may recognize that this is exactly the <strong>Evidence Lower Bound (ELBO)</strong>. Here “evidence” refers to the observed data, and it is called the ELBO because it’s a lower bound. Note that the ELBO depends on two things: the unknown parameter $\theta$ and the distribution function for the latent variable $q(z)$. Now, EM arises from the following two insights.</p> <p>First, for a given $q(z)$, finding the $\theta$ that maximizes the ELBO is much easier than optimizing marginal likelihood. Let’s call the first component of this lower bound</p> \[Q(\theta) = E_{q(z)} [\log p(x,z|\theta)] = \sum_{z\in\mathcal{Z}^n} q(z) \log p(x,z|\theta)\] <p>Since the second term $H(q)$ (the entropy of function $q$) does not depend on $\theta$, maximizing $Q(\theta)$ sufficient to maximize the ELBO. Notice that now the log is inside the sum, and taking derivatives with respect to $\theta$ is much easier! For this reason, we can often find <em>analytical</em> solutions for maximizing the ELBO as a function of $\theta$.</p> <p>Second, given a fixed $\theta$, finding the $q(z)$ that maximizes the ELBO is also easy. To see this, let’s rewrite the ELBO in a different form:</p> \[\begin{align*} \text{ELBO}(\theta, q) &amp;= E_{q(z)}\Big[\log \Big( \frac{p(x,z|\theta)}{q(z)} \Big) \Big] \\ &amp;= E_{q(z)}\Big[\log \Big( \frac{p(x,z|\theta)}{p(z|x,\theta)} \frac{p(z|x,\theta)}{q(z)} \Big) \Big] \\ &amp;= E_{q(z)}\Big[\log \Big( \frac{p(z|x,\theta)p(x|\theta)}{p(z|x,\theta)} \frac{p(z|x,\theta)}{q(z)} \Big)\Big] \\ &amp;= E_{q(z)}\Big[\log \Big( p(x|\theta) \frac{p(z|x,\theta)}{q(z)} \Big)\Big] \\ &amp;= E_{q(z)}[\log p(x|\theta)] - E_{q(z)}\Big[\log\Big( \frac{q(z)}{p(z|x,\theta)} \Big)\Big] \\ &amp;= \log p(x|\theta) - \text{KL}(q || p(z|x,\theta)) \end{align*}\] <p>Here we can see that the gap between the ELBO and the marginal log-likelihood is exactly the <strong>Kullback–Leibler (KL)</strong> divergence between $q$ and $p(z|x,\theta)$, a.k.a the posterior of the latent variable conditioned on the observed data and the parameter $\theta$. Since KL between two distributions is non-negative and is only 0 when they are exactly the same, it becomes apparent that the choice of $q(z)$ that maximizes the ELBO is precisely $p(z|x,\theta)$.</p> <p>Combining the above two observations, we arrive at an iterative procedure to obtain ML estimate of $\theta$, which alternates between optimizing the ELBO with respect to $\theta$ while holding $q(z)$ fixed, then with respect to $q(z)$ while holding $\theta$ fixed.</p> <ol> <li>Initialize $\theta = \theta^{(0)}$.</li> <li>(<strong>E-step</strong>) Compute the posterior of the latent variables $p(z | x,\theta^{(t)})$ and use it to compute $Q(\theta)$.</li> <li>(<strong>M-step</strong>) Find new $\theta$ estimate that maximizes $Q(\theta)$; i.e., $\theta^{(t+1)} = \arg \max_\theta Q(\theta)$.</li> </ol> <p>The second step is called the E-step because it involves the computation of $Q$, which is an expectation. The third step is called the M-step because it is maximizing the objective function $Q(\theta)$. Note that $Q$ can be viewed as the same function across time steps but configured with different $\theta^{(t)}$s, so we often write it as $Q(\theta; \theta^{(t)})$, a function of both $\theta$ and $\theta^{(t)}$.</p> <p>The E-step is in some way also a maximization step, because choosing $q(z) = p(z | x,\theta^{(t)})$ maximizes the ELBO at $\theta = \theta^{(t)}$ (in fact, it is exactly equal to $l(\theta^{(t)})$, the log marginal likelihood at $\theta = \theta^{(t)}$). In this way, EM can be viewed as a special case of <a href="https://en.wikipedia.org/wiki/Coordinate_descent">coordinate ascent</a>.</p> <h3 id="visualization">Visualization</h3> <p>Here is a graphical illustration of how the iterative update works:</p> <center> <img src="/assets/img/EM1.png" alt="drawing" style="width:700px;"/> </center> <p><strong>Figure 1</strong>. EM parameter update at iteration $t$. Two functions are plotted: the log marginal likelihood $l(\theta)$ and the ELBO computed using $q_t(z)=p(z|x,\theta^{(t)})$. The gap between $l(\theta)$ and the ELBO is the KL divergence between $q_t(z)$ and $p(z|x;\theta)$. In the M-step, we move from point A to point B by optimizing ELBO with respect to $\theta$. In the E-step, we maximize the ELBO at the updated $\theta$ by updating the $q(z)$ function, moving from point B to point C.</p> <h3 id="proof-of-correctness">Proof of correctness</h3> <p>How do we know whether EM is guaranteed to converge? It can be proved that the marginal likelihood monotonically increases with each parameter update. i.e.,</p> \[l(\theta^{(0)}) \leq l(\theta^{(1)}) \leq l(\theta^{(2)}) \leq \cdots\] <p>From the diagram above, we can intuitively see why this is true. The three points that we have marked on the graph respectively represent three key quantities:</p> \[\begin{align*} A&amp;: \text{ELBO}(\theta^{(t)},q_t)=l(\theta^{(t)}) \\ B&amp;: \text{ELBO}(\theta^{(t+1)},q_t) \\ C&amp;: l(\theta^{(t+1)}) \end{align*}\] <p>Intuitively, B is always higher than A, and C is always higher than B in the graph. More formally,</p> \[l(\theta^{(t+1)}) \geq \text{ELBO}(\theta^{(t+1)},q_t) \geq \text{ELBO}(\theta^{(t)},q_t)=l(\theta^{(t)})\] <p>Where the first inequality is because ELBO is always a lower bound of $l$, the second is because we choose the new $\theta$ to be the maximizer of the current ELBO, and the final inequality is that $q_t$ was chosen so that the current ELBO is exactly equal to $l$ at the current $\theta$. Therefore, each time we update $\theta$ (M-step) we attain a higher marginal likelihood.</p> <p>On the other hand, when we update $q(z)$ while keeping $\theta$ fixed (E-step), we choose $q_{t+1}$ such that we make the ELBO exactly equal to $l$ at the current $\theta$. So,</p> \[\text{ELBO}(\theta^{(t)},q_{t+1}) = l(\theta^{(t)}) \geq \text{ELBO}(\theta^{(t)},q_t)\] <p>so each time we update $q(z)$ we make the ELBO a tighter bound of $l$ at the current $\theta$.</p> <p>We can now appreciate that with a few lines of derivations, we have achieved something significant: we have derived and proved a general class of EM algorithms that can be applied to a wide range of statistical models.</p> <h2 id="deriving-an-em-algorithm">Deriving an EM algorithm</h2> <h3 id="general-procedure">General procedure</h3> <p>Based on the framework above, we have the following steps for deriving the EM update rules for specific inference problem. To be more general, let’s use $\Theta$ to denote the collection of parameters in a model.</p> <ol> <li>To derive the E-step (finding the lower bound), write down the conditional posterior $p(z|x,\Theta^{(t)})$ and the $Q(\Theta; \Theta^{(t)})$ function:</li> </ol> \[Q(\Theta; \Theta^{(t)}) = E_{p(z|x,\Theta^{(t)})} [\log p(x,z|\Theta)]\] <ol> <li>To drive the M-step (updating parameters to maximize the lower bound), find an analytical formula for $\Theta$ such that \(\Theta^{(t+1)} = \arg\max_\Theta Q(\Theta; \Theta^{(t)})\)</li> </ol> <p>This gives the parameter update rule.</p> <h3 id="example-derivation">Example derivation</h3> <p>Let’s go over a concrete example to see how EM derivation works in action. Let’s say we are trying to infer the failure rates of light bulbs coming from different manufacturing batches. Let $x_i$ be the observed failure time of lightbulb $i \in {1\ldots n}$, and $z_i \in {1, \ldots, K}$ be the batch that lightbulb $i$ came from (which we do not observe). Let’s model the failure time using an exponential distribution, where the failure rate is different for each batch $k$:</p> \[x_i |z_i = k \overset{ind}{\sim} \text{Expo}(\lambda_k)\] <p>The batch itself is drawn from a categorial distribution:</p> \[z_i \overset{i.i.d.}{\sim} \text{Categorical}(\pmb{\pi})\] <p>To make the problem more challenging, let’s say both $\pmb{\lambda} = (\lambda_1, \lambda_2, \ldots, \lambda_K)$ and $\pmb{\pi} = (\pi_1, \pi_2, \ldots, \pi_K)$ are unknown. Let’s denote the collection of these model parameters as $\Theta = (\pmb{\lambda}, \pmb{\pi})$. Well, how do we figure out the failure rates of different light bulb batches, if the batch labels themselves are unknown? Let’s see how we can derive an EM algorithms that can achieve this.</p> <p>First, let’s derive the E-step.</p> \[\begin{align*} Q(\Theta; \Theta^{(t)}) &amp;= E_{z} [\sum_i \log p(x_i,z_i|\Theta)] \\ &amp;= \sum_i E_{z} [\log p(x_i|z_i,\pmb{\lambda}) p(z_i|\pmb{\pi})] \\ &amp;= \sum_i \{ E_{z} [\log p(x_i|z_i,\pmb{\lambda})] + E_z[ \log p(z_i|\pmb{\pi})] \} \end{align*}\] <p>We can see that the $Q$ function is broken up into two main components, which are respectively:</p> \[\begin{align*} \sum_i E_{z} [\log p(x_i|z_i,\pmb{\lambda})] &amp;= \sum_i \sum_k p(z_i=k|x_i,\Theta^{(t)}) \log p(x_i|z_i=k,\lambda_k) \\ &amp;= \sum_i \sum_k \phi^t_{z_i}(k) (\log \lambda_k - \lambda_k x_i) \end{align*}\] \[\begin{align*} \sum_i E_z[ \log p(z_i|\pmb{\pi})] &amp;= \sum_i \sum_k p(z_i=k|x_i,\Theta^{(t)}) \log \pi_k \\ &amp;= \sum_i \sum_k \phi^t_{z_i}(k) \log \pi_k \end{align*}\] <p>Where we have defined $\phi^t_{z_i}(k) = p(z_i=k|x_i,\Theta^{(t)})$, which is intuitively the membership probability that lightbulb $x_i$ belongs to batch $k$. Note that this quantity does not dependent on $\Theta$ (it only depends on $\Theta^{(t)}$, which is treated as a known constant). Also, $\sum_k \phi^t_{z_i}(k) = 1$ since it is a conditional probability. We can compute it using Bayes’ Rule:</p> \[\begin{align*} \phi^t_{z_i}(k) &amp;= p(z_i=k|x_i,\Theta^{(t)}) \\ &amp;= \frac{p(x_i|z_i=k,\lambda^t_k)p(z_i = k|\pi^t_k)}{\sum_k p(x_i|z_i=k,\lambda^t_k)p(z_i = k|\pi^t_k)} \\ &amp;= \frac{\lambda_k^t \text{exp}(-\lambda_k^t x_i) \pi_k^t}{\sum_k \lambda_k^t \text{exp}(-\lambda_k^t x_i) \pi_k^t} \end{align*}\] <p>All together, we have</p> \[Q^t(\Theta) = \sum_i \sum_k \phi^t_{z_i}(k) (\log \lambda_k - \lambda_k x_i) + \sum_i \sum_k \phi^t_{z_i}(k) \log \pi_k\] <p>which is a function of $\pmb{\lambda},\pmb{\pi}$ and $\pmb{\lambda}^{(t)},\pmb{\pi}^{(t)}$.</p> <p>Now, let’s derive the M-step. Taking partial derivatives with respect to each $\pi_k$ parameter, while adding a Lagrange multiplier $\alpha$ for the constraint $\sum_k \pi_k = 1$, we have:</p> \[\frac{\partial (Q^t + \alpha(1-\sum_k \pi_k))}{\partial \pi_k} = \sum_i \frac{\phi^t_{z_i}(k)}{\pi_k} - \alpha \overset{\Delta}{=} 0 \\ \implies \pi_k = \frac{\sum_i \phi^t_{z_i}(k)}{\alpha}\] <p>Since $\sum_k \pi_k = 1$ and $\phi^t_{z_i}(k) = 1$,</p> \[\begin{align*} \frac{\sum_k \sum_i \phi^t_{z_i}(k)}{\alpha} &amp;= 1 \\ \alpha &amp;= \sum_i \sum_k \phi^t_{z_i}(k) = \sum_i 1 = n \end{align*}\] <p>So this means that the parameter update for $\pi_k$ is</p> \[\pi_k = \frac{\sum_i \phi^t_{z_i}(k)}{n}\] <p>For $\lambda_k$, take partial derivative again:</p> \[\frac{\partial Q^t}{\partial \lambda_k} = \sum_i \phi^t_{z_i}(k) (\lambda_k^{-1}-x_i) \overset{\Delta}{=} 0 \\ \implies \lambda_k = \frac{\sum_i \phi^t_{z_i}(k) }{\sum_i \phi^t_{z_i}(k)x_i }\] <p>Putting the two steps together, we have the following iterative update rules:</p> <ol> <li>Initialize $\pi^{(0)}_k = 1/K$ and $\lambda^{(0)}_k = 1$.</li> <li>(E-step) Compute $\phi^t_{z_i}(k)$ and $Q^t(\Theta)$, where</li> </ol> \[\phi^t_{z_i}(k) = \frac{\lambda_k^t \text{exp}(-\lambda_k^t x_i) \pi_k^t}{\sum_k \lambda_k^t \text{exp}(-\lambda_k^t x_i) \pi_k^t}\] <ol> <li>(M-step) Update parameters</li> </ol> \[\begin{align*} \lambda^{(t+1)}_k &amp;= \frac{\sum_i \phi^t_{z_i}(k) }{\sum_i \phi^t_{z_i}(k)x_i} \\ \pi^{(t+1)}_k &amp;= \frac{\sum_i \phi^t_{z_i}(k)}{n} \end{align*}\] <p>which works for any exponential mixtures. Note that the $Q$ function does not need to be explicitly computed, because we have an analytical solution for maximizing it (which is our M-step).</p> <h3 id="implementation">Implementation</h3> <p>Here is an implementation of the EM algorithm we just derived (in R).</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span><span class="w">

</span><span class="n">get_phi_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">lambda_t</span><span class="p">,</span><span class="w"> </span><span class="n">pi_t</span><span class="p">,</span><span class="w"> </span><span class="n">x_i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">k</span><span class="p">){</span><span class="w">
        </span><span class="n">lambda_t</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">lambda_t</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x_i</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">pi_t</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w">
    </span><span class="p">})</span><span class="w">
    </span><span class="n">phi_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">l</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">l</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="n">phi_k</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">update_lambda_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">phi_k</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">lambda_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">phi_k</span><span class="p">)</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">phi_k</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="n">lambda_k</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">update_pi_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">phi_k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">pi_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">phi_k</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="w">
    </span><span class="nf">return</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">run_em</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">max_iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    
    </span><span class="c1"># initialization</span><span class="w">
    </span><span class="nb">pi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">()</span><span class="w">
    </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">()</span><span class="w">
    </span><span class="nb">pi</span><span class="p">[[</span><span class="m">1</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">)</span><span class="w">
    </span><span class="n">lambda</span><span class="p">[[</span><span class="m">1</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="w">
    
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="p">(</span><span class="n">max_iter</span><span class="m">-1</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
        
        </span><span class="c1"># E-step</span><span class="w">
        </span><span class="n">phi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="n">sapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
                </span><span class="n">get_phi_k</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="p">[[</span><span class="n">t</span><span class="p">]],</span><span class="w"> </span><span class="nb">pi</span><span class="p">[[</span><span class="n">t</span><span class="p">]],</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w">
            </span><span class="p">})</span><span class="w">
        </span><span class="p">})</span><span class="w">
        
        </span><span class="c1"># M-step</span><span class="w">
        </span><span class="n">lambda</span><span class="p">[[</span><span class="n">t</span><span class="m">+1</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="n">update_lambda_k</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">phi</span><span class="p">[[</span><span class="n">k</span><span class="p">]],</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w">
        </span><span class="p">})</span><span class="w">
        
        </span><span class="nb">pi</span><span class="p">[[</span><span class="n">t</span><span class="m">+1</span><span class="p">]]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sapply</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="n">update_pi_k</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">phi</span><span class="p">[[</span><span class="n">k</span><span class="p">]])</span><span class="w">
        </span><span class="p">})</span><span class="w">
                             
    </span><span class="p">}</span><span class="w">
                             
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lambda</span><span class="p">,</span><span class="w"> </span><span class="nb">pi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">pi</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <p>Let’s simulate some data, with 3 batches each with different failure rates.</p> <div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">2</span><span class="p">)</span><span class="w">

</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="w">
    </span><span class="n">rgamma</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w">
    </span><span class="n">rgamma</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">300</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w">
    </span><span class="n">rgamma</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">200</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">sort</span><span class="w">

</span><span class="n">n_iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="w">
</span><span class="n">res</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">run_em</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">max_iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n_iter</span><span class="p">)</span><span class="w">
</span></code></pre></div></div> <center> <img src="/assets/img/EM2.png" alt="drawing" style="width:700px;"/> </center> <p><strong>Figure 2</strong>. <strong>A</strong>, Distribution of the simulated data. <strong>B</strong>, log-model likelihood across iterations. <strong>C</strong> and <strong>D</strong>, parameter estimates at each iteration. Red triangles mark the true parameter values.</p> <p>We see that the algorithm converges after 50 iterations or so, with monotonically improving model fit. Even though we can’t spot any obvious batch effects in the distribution of the data, the failure rate and prevalence of each batch estimated by EM are pretty much spot-on!</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>Thanks to Hirak Sarkar for providing helpful feedback. The illustration in Figure 1 was largely inspired by this <a href="https://mbernste.github.io/posts/em/">post</a>.</p> <h2 id="references-and-further-readings">References and further readings</h2> <ol> <li> <p><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm</a></p> </li> <li> <p><a href="https://mbernste.github.io/posts/em/">https://mbernste.github.io/posts/em/</a></p> </li> <li> <p><a href="https://hiraksarkar.github.io/posts/2021/06/theory-of-vae/">https://hiraksarkar.github.io/posts/2021/06/theory-of-vae/</a></p> </li> <li> <p><a href="http://www.cs.cmu.edu/~awm/10701/assignments/EM.pdf">http://www.cs.cmu.edu/~awm/10701/assignments/EM.pdf</a></p> </li> </ol>]]></content><author><name>Teng Gao</name></author><category term="tutorial"/><category term="statistics"/><category term="machine-learning"/><summary type="html"><![CDATA[From theory to implementation]]></summary></entry></feed>